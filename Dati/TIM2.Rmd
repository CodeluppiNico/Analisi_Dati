---
title: "Analisi di Dati Telecom"
author: "Marco Mamei"
date: "7-7-2017"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = F)
output = knitr::opts_knit$get("rmarkdown.pandoc.to") 
# Imposto una voarabile per decidere che tipo di output usare in knit (html, latex)
```

## Importazione di Librerie

Importiamo le librerie necessarie:

```{r, echo=TRUE, warning=FALSE,error=FALSE,message=FALSE}

# serie temporali
library(lubridate)
library(zoo)
library(tseries)

# operazioni su data-frame
library(dplyr)
library(tidyr)
library(broom)

# machine learning (modelli per le serie temporali, e computazione multicore)
library(forecast)
library(parallel)
library(vars)

# visualizzazione
library(ggplot2)
library(ggmap)
library(corrplot)
library(rgdal)
library(sp)
library(maptools)
library(leaflet)
library(leaflet.minicharts)
library(RColorBrewer)



```

## Importazione Dati

Carichiamo il file in un data frame


```{r}
#file = "Modena_20170601_20170711"
#file = "Modena_20110416_20110417"
file = "Modena_20170416_20170417"
data = read.csv(file,header=FALSE,stringsAsFactors=FALSE, na.strings = c("65535"))
names(data) = c("time","cell","value")
data$rtime = as.POSIXct(fast_strptime(data$time, "%Y%m%d_%H%M"))
head(data)
```

## Visualizziamo la mppa della città.

Carico il file SHP con la mappa della città. 
Questo file si può aprire anche con QGIS.

Visualizzo la girglia sulla città tramite la libreria leaflet.
La visualizzazione con il packge leaflet è compatibile solo con l'output Knit in HTML.
Posso cliccare sui vari "pixel" per scoprire il loro ID.

```{r}



shpFile = "Modena.shp"
z <- readShapePoly(shpFile,IDvar="SP_ID")

if(is.null(output) || output == "html") {
  leaflet(z) %>%
  addPolygons(stroke = TRUE, color = "#000", weight=1, 
              fillOpacity = 0.5, fillColor = "#03F", smoothFactor = 0.5,
              popup = ~SP_ID) %>%
  addTiles() #adds a map tile, the default is OpenStreetMap
} else {
  plot(z)
}

```


## Visualizziamo le serie temporali di alcuni pixel sul territorio.

Seleziono il pixel 398-1038

```{r}

id = "611-1240"
x = ts(filter(data,cell==id)$value, freq=7*4*24)
x = x / 10 * 3
plot(x,main=id,ylab="num. people",xlab="time (weeks)")

```

Vedo che il grafico presenta molti "buchi" valori NA.



```{r}
sum(is.na(x))
length(x)
```

Elimino i valri NA per interpolazione


```{r}
# ?na.approx
id = "611-1240"
x = ts(filter(data,cell==id)$value, freq=7*4*24)
x = x / 10 * 3
x = na.approx(x)
plot(x,main=id,ylab="num. people",xlab="time (weeks)")

```

Vediamo altri pixel


```{r}
id = "392-1040"
x = ts(filter(data,cell==id)$value, freq=7*4*24)
x = x / 10 * 3
x = na.approx(x)
plot(x,main=id,ylab="num. people",xlab="time (weeks)")

id = "394-1028"
x = ts(filter(data,cell==id)$value, freq=7*4*24)
x = x / 10 * 3
x = na.approx(x)
plot(x,main=id,ylab="num. people",xlab="time (weeks)")

plot(window(x,start=c(1,1),end=c(3,480)),main=id,ylab="num. people",xlab="time (weeks)")

```



## Identificazione Eventi

Creiamo una semplice procedura automatica per trovare eventi (i.e., situazioni anomale sovraaffollate) in una cella.
(contributo di Federico Berlingeri)

```{r, echo=FALSE}

id = "394-1028"
x = ts(filter(data,cell==id)$value, freq=7*4*24)
x = x / 10 * 3
x = na.approx(x)

# sono i punti che sono x : x > Q75 + coef * IQR e x < Q25 - coef  * IQR (coef = 1.5 default)  

outlier = boxplot.stats(x)$out

y = ifelse(x %in% outlier, 1 , 0)

plot(x)
par(new = T)
plot(y,axes=F, xlab=NA, ylab=NA)


```


## Visualizziamo la mappa della città (Colorpleth)

Visualizzo la città colorando con secondo il numero di persone presenti

il nostro oggetto z è della classe "SpatialPolygonsDataFrame" contiene diversi campi accessibili tramite il carattere @.
In particolre z@data permette di accedere ai dati nel data frame vero e proprio. Ogni riga del dataframe è associata ad un elemento geografico (z@polygons fa vedere invece le coordinate).

Devo fare un join tra z@data e data in modo da avere in un unico oggetto le coodinate geografiche e i dati di popolazione


```{r,warning=FALSE,error=FALSE,message=FALSE}
z <- readShapePoly(shpFile,IDvar="SP_ID")
head(z@data)
head(data)

data_day = filter(data,time=="20110416_1100")
z@data$cell = as.character(z@data$SP_ID) # z@data$SP_ID è un factor e non riuscirei a fare il join 
#?left_join
z@data = z@data %>% left_join(data_day) %>% dplyr::select(-val)
head(z@data)

qpal = colorNumeric("Blues",z@data$value,na.color = "#FFFFFF")

if(is.null(output) || output == "html") {
  leaflet(z) %>%
  addPolygons(stroke = TRUE, color = qpal(z$value), weight=1, fillOpacity = 0.5, 
              smoothFactor = 0.5,popup= paste(z$cell," value ",z$value)) %>%
  addTiles() %>% 
  #addTiles(urlTemplate="http://stamen-tiles-{s}.a.ssl.fastly.net/toner/{z}/{x}/{y}.png") %>%
  addLegend("bottomright", pal = qpal, values = ~value,title = "n. persons",opacity = 1)
} else {
  #plot(z,col= qpal(z$value))
  bbox = data.frame(10.886880,44.617892,10.953896,44.657667)
  tmap = tidy(z)
  tmap = tmap %>% inner_join(data_day,by=c("id"="cell"))
  names(bbox) = c("ll.lon","ll.lat","ur.lon","ur.lat")
  amap = get_map(location = z@bbox, source="stamen", maptype="toner",color="bw",crop=TRUE)
  ggmap(amap, extent = "normal", maprange = FALSE) +
  #ggplot()+
  geom_polygon(data = tmap,
               aes(long, lat, group = group, fill=value),
               colour = "black", alpha = 0.5) +
  theme_bw() +
  scale_fill_distiller(palette='Blues',direction=1)+
  coord_map(projection="mercator",
            xlim=c(attr(amap, "bb")$ll.lon, attr(amap, "bb")$ur.lon),
            ylim=c(attr(amap, "bb")$ll.lat, attr(amap, "bb")$ur.lat)) 
  
}

```


## Vogliamo visualizzare dei flussi sulla mappa


```{r}
flux = read.csv('flux_1600-1615_4x4.csv',header=TRUE,stringsAsFactors=FALSE)
flux = flux %>% filter(from != to & qty > 0)
z <- readShapePoly(shpFile,IDvar="SP_ID")
z@data$cell = as.character(z@data$SP_ID)
#z@data = z@data %>% left_join(data_day) %>% dplyr::select(-val)

centroids = as.data.frame(getSpPPolygonsLabptSlots(z))
names(centroids) = c('lon','lat')
centroids = cbind(centroids,z@data %>% dplyr::select(-c(val,SP_ID)))
flux2 = flux %>% inner_join(centroids %>% rename(from.lat = lat, from.lon=lon), by = c("from" = "cell")) %>% inner_join(centroids %>% rename(to.lat = lat, to.lon=lon), by = c("to" = "cell"))


data_day = filter(data,time=="20170416_1100")
z@data = z@data %>% left_join(data_day) %>% dplyr::select(-val)

qpal = colorNumeric("Blues",z@data$value,na.color = "#FFFFFF")

output = 'html'
if(is.null(output) || output == "html") {
  leaflet(z) %>%
  addPolygons(stroke = TRUE, color = qpal(z$value), weight=1, fillOpacity = 0.5, 
              smoothFactor = 0.5,popup= paste(z$cell," value ",z$value)) %>%
  addFlows(flux2$from.lon,flux2$from.lat,flux2$to.lon,flux2$to.lat,flow = flux2$qty, maxThickness = 5) %>%
    addTiles() %>% 
  #addTiles(urlTemplate="http://stamen-tiles-{s}.a.ssl.fastly.net/toner/{z}/{x}/{y}.png") %>%
  addLegend("bottomright", pal = qpal, values = ~value,title = "n. persons",opacity = 1)
} else {
  #plot(z,col= qpal(z$value))
  bbox = data.frame(10.886880,44.617892,10.953896,44.657667)
  tmap = tidy(z)
  tmap = tmap %>% inner_join(data_day,by=c("id"="cell"))
  names(bbox) = c("ll.lon","ll.lat","ur.lon","ur.lat")
  amap = get_map(location = z@bbox, source="stamen", maptype="toner",color="bw",crop=TRUE)
  ggmap(amap, extent = "normal", maprange = FALSE) +
  #ggplot()+
  geom_polygon(data = tmap,
               aes(long, lat, group = group, fill=value),
               colour = "black", alpha = 0.5) +
  theme_bw() +
  scale_fill_distiller(palette='Blues',direction=1)+
  coord_map(projection="mercator",
            xlim=c(attr(amap, "bb")$ll.lon, attr(amap, "bb")$ur.lon),
            ylim=c(attr(amap, "bb")$ll.lat, attr(amap, "bb")$ur.lat))+
  geom_segment(aes(x = from.lon, y = from.lat, xend = to.lon, yend = to.lat), size=2*flux2$qty/max(flux2$qty), arrow = arrow(length = unit(0.1,"cm")), data = flux2)
}


```



## Vogliamo provare a predire il numero di persone su una cella.

Utilizziamo un modello lineare.
Chiamo F (dal parametro freq di ts) il numero di seasonal periods nella serie temporale.


$$
y_t = \beta_0 + \beta_1 \cdot t + \sum_i^{F} \beta_i \cdot d_{it}
$$

$d_{it} =  1$ se $t$ è nel periodo $i$, $0$ altrimenti.   


La funzione tslm è come la funzione lm, ma permette di creare in modo automatico le variabili $d_{it}$ associate a tutti i seasonal periods della serie temporale. 

Per chi vuole approfondire questo è un libro veramente fantastico:

https://www.otexts.org/fpp

https://www.otexts.org/fpp2 (2 edizione, in corso di completamento)






```{r}

id = "398-1038"
y = ts(filter(data,cell==id)$value, freq=7*4*24)
y = y / 10 * 3
y = na.approx(y)

#fit = stlf(x)
#fit = stlf(sts,method="arima")
fit <- tslm(y ~ trend + season)
#summary(fit)
plot(forecast(fit,h=3*7*24*4))
accuracy(fit)



days.ts =ts(matrix(0,ncol=7,nrow=length(y)),freq=7*24*4)
for(i in 1:7) {
  days.ts[,i] = cycle(y) >= (i-1)*24*7 & cycle(y) < i*24*7
}
fit <- tslm(y ~ days.ts)
plot(forecast(fit,h=length(y)))



h.ts =ts(matrix(0,ncol=24,nrow=length(y)),freq=24*4)
for(i in 1:24) {
  h.ts[,i] = cycle(h.ts[,i]) >= (i-1)*4 & cycle(h.ts[,i]) < i*4
}
fit <- tslm(y ~ h.ts)
plot(forecast(fit,h=length(y)))


```

Ci sono molte covariate (674). Potremmo sicuramente usarne molte meno, ma a quel punto dovremmo costruirci noi le variabili seasonal.


Abbiamo valitato l'accuracy sul training set.
Dividiamo in training e test set.



```{r}

id = "398-1038"
y = ts(filter(data,cell==id)$value, freq=7*4*24)
y = y / 10 * 3
y = na.approx(y)

train = window(y,end=c(4,480))
test = window(y,start=c(4,480))

#fit = stlf(train)
#fit = stlf(train,method="arima")
fit <- tslm(train ~ trend + season)
#summary(fit)
fcast = forecast(fit,h=length(test))
plot(fcast)
lines(y)
accuracy(fcast,test)
#mean(abs(fcast$mean - test))

boxplot(abs(fcast$mean - test))

```

## Siamo stati fortunati nella scelta della cella?


Vogliamo applicare la classificazione a tutte le celle.
R non gestisce i cicli (es. for) in modo efficace, ma usa le funzioni vettorizzate.
Dobbiamo defnine una funzione che effettui la predizione su una cella. 
Quindi usare una delle funzioni apply di R per applicare la funzione di predizione a tutti gli elementi


Per eseguire la funzione più velocemente, creiamo un sample delle celle


```{r}

cells = data %>% distinct(cell) %>% dplyr::select(cell)
sample = sample(cells$cell,10)
ss = as.data.frame(sample,stringsAsFactors = F)
names(ss) = c("cell")
data.sample = semi_join(data,ss)


fcast = function(cellx) {
  df = filter(data.sample,cell==cellx) %>% dplyr::select(rtime,value)
  y = ts(na.approx(df[,2]), freq=7*4*24)
  train = window(y,end=c(4,480))
  test = window(y,start=c(4,480))
  fit <- tslm(train ~ trend + season)
  fcast = forecast(fit,h=length(test))
  #fit.e = stlf(train)
  #fit.a = stlf(train,method="arima")
  acc = accuracy(fcast,test)
  acc1 = matrix(c(acc[1,],acc[2,]),1,16,byrow = TRUE)
  colnames(acc1) = c(colnames(acc),paste("tst",colnames(acc),sep="."))
  return(data.frame(cellx,acc1,stringsAsFactors = F)) 
}

res1 = fcast(sample[1])
res = lapply(sample,fcast) # for each cell in sample apply fcast
resall = res %>% rbind_all()

boxplot(resall$tst.MAE)



```

Analizziamo meglio gli outlier di questo box plot


```{r}

out = filter(resall,tst.MAE==max(resall$tst.MAE))



id = out$cellx
y = ts(filter(data,cell==id)$value, freq=7*4*24)
y = y / 10 * 3
y = na.approx(y)

train = window(y,end=c(4,480))
test = window(y,start=c(4,480))

#fit = stlf(train)
#fit = stlf(train,method="arima")
fit <- tslm(train ~ trend + season)
#summary(fit)
fcast = forecast(fit,h=length(test))
plot(fcast)
lines(y)

boxplot(abs(fcast$mean - test))

```

## Relazione tra le celle


Una possibilità interessante per migliorare la predizione è di 
sfruttare la correlazione spaziale tra celle diverse. 
Ad esempio, un picco di traffico allo stadio potrebbe generare un picco di traffico un'ora dopo alla stazione dei treni.
Questa idea è analizzata in parte in:

Discovering urban and country dynamics from mobile phone data with spatial correlation patterns
R. Trasarti, A. Olteanu-Raimond, M. Nanni, T. Couronné, B. Furletti, F. Giannotti, Z. Smoreda, C. Ziemlicki
Telecommunication Policy 2014


Come prima cosa vogliamo analizzare la correlazione spaziala tra le celle.
L'idea è duplice.
Da un parte vogliamo verificare la correlazione è una funzione della distanza (quindi se celle vicine si influenzano di più).
Più in generale vogliamo torvare le celle che si influenzano di più in modo da creare un modello in cui cerchiamo di prevedere una serie y
con anche le serie che correlano meglio e che quindi dovrebbero avere più informazione.


```{r}

# L'ho già fatto prima

# Otteniamo un sample dei dati di 10 celle
#cells = data %>% distinct(cell) %>% dplyr::select(cell)
#sample = sample(cells$cell,10)
#ss = as.data.frame(sample,stringsAsFactors = F)
#names(ss) = c("cell")
#data.sample = semi_join(data,ss)




# applichiamo stl a tutte le celle. Uso una t.window molto bassa (un'ora) perchè dopo confronterò le parti di trend 
allstl = lapply(sample,function(id) {
  s = ts(filter(data.sample,cell==id)$value,freq=7*4*24)
  stl = stl(na.approx(s),s.window = "periodic", t.window = 4)
  list(id,stl)
})
allstl = setNames(allstl,sample)


# vediamo cosa produce stl alla serie temporale. L'idea è di vedere correlazioni nei trend
plot(allstl[[1]][[2]])

## vediamo la correlazione tra due serie
t1 = allstl[[sample[1]]][[2]]$time.series[,"trend"]
t2 = allstl[[sample[2]]][[2]]$time.series[,"trend"]
plot(ccf(t1,t2))

# vedo che la correlazione di una serie rispetto all'altra.

# applico la stessa cosa. Per ogni serie, guardo la ccf con tutte le altre.
# Per ogni coppia i,j trovo il valore massimo di correlazione considerando solo la prima metà
# quella a lag negativi (in modo tra torvare la correlazione di una cella nei confronti del passato # delle altre)

maxccf = sapply(sample,function(id1) {
  print(id1)
  (sapply(sample,function(id2) {
    
          if(id1 == id2) return(0)
    
          t1 = allstl[[id1]][[2]]$time.series[,"trend"]
          t2 = allstl[[id2]][[2]]$time.series[,"trend"]
          #print(length(t1))
          #print(length(t2))
          ccf = ccf(t1,t2,plot=FALSE)
          max(abs(ccf$acf[1:floor(length(ccf$acf)/2)]))
  }))
})

# il risultato è una matrice che per ogni i,j dice il massimo della correlazione trovata.
# Vediamo quanto erano distanti quelle celle


# NOTA: C'è un bug sulle celle. A volte seleziona una cella x-1015 (es. 411-1015) che non è in mappa (la mappa inizia da 
# x-1016) !!!!!!!!

shpFile = "Modena.shp"
map <- readShapePoly(shpFile,IDvar="SP_ID")
coords<-coordinates(map)
coords = coords[sample,]
d = spDists(coords,longlat = TRUE)
rownames(d) = rownames(coords)
colnames(d) = rownames(coords)

par(mfrow=c(1,2))
corrplot(maxccf,is.corr=FALSE,title="max CCF")

d = max(d) - d

corrplot(d,is.corr=FALSE,title="nearness (k-distance)")


par(mfrow=c(1,1))

plot(data.frame(maxccf=as.vector(maxccf),dist.km=as.vector(d)),pch=16)


```

From the above graph you can see that distance does not correlate well with ccf (for max negative lag).
But there are examples of strong spatial correlations among cells.


## Classification using other cells

We try classification with the value of other cells as covariates. 
global VAR models do not scale with so many cells.
We can use a VAR models only with cells above a given CCF

```{r}


fcast.var = function(id) {
  #id = sample[1]
  print(id)
  id.ts = ts(filter(data.sample,cell==id)$value, freq=7*4*24)
  
  maxccf.id = maxccf[id,]
  maxccf.id = maxccf.id[maxccf.id > 0.8]
  others = names(maxccf.id)
  
  group.ts <- ts(matrix(0, ncol = 1+length(others), nrow = length(id.ts)),freq=7*4*24)
  colnames(group.ts) = c(id,others)
  group.ts[,1] = id.ts
  if(length(others) > 0) {
    for(i in 1:length(others)){
      tsi = ts(filter(data.sample,cell==others[i])$value,freq=7*4*24)
      #tsi = seasadj(decompose(na.approx(tsi,na.rm=F)))
      #print(length(tsi))
      group.ts[,(i+1)] = tsi
    }
  }
  #plot(na.approx(group.ts))
  
  # I can use a VAR for this group
  
  train = window(group.ts,end=c(end(group.ts)[1]-1,1))
  test = window(group.ts,start=c(end(group.ts)[1]-1,2))
  
  print(paste(id,"************************",dim(train)[2]))
  fit = NULL
  if(dim(train)[2] > 1) {
    fit <- VAR(na.approx(train), p = 2, season=7*4*24)
    #var <- VAR(na.approx(group.ts,na.rm=T), p = 2)
  } else { 
    fit = stlf(na.approx(train))
  }
  fcast = forecast(fit,h=1000)
  #plot(fcast)
  a = accuracy(fcast,test, d=0,D=0)
  acc1 = matrix(c(a[1,],a[2,]),1,16,byrow = TRUE)
  colnames(acc1) = c(colnames(a),paste("tst",colnames(a),sep="."))
  return(data.frame(id,acc1,stringsAsFactors = F)) 
}


# prepare also a simple fcast function for comparison

fcast = function(id) {
  df = filter(data.sample,cell==id) %>% dplyr::select(rtime,value)
  sts = ts(na.approx(df[,2]), freq=7*4*24)
  train = window(sts,end=c(end(sts)[1]-1,1))
  test = window(sts,start=c(end(sts)[1]-1,2))
  fit = stlf(train)
  #fit = stlf(train,method="arima")
  acc = accuracy(fit,test)
  acc1 = matrix(c(acc[1,],acc[2,]),1,16,byrow = TRUE)
  colnames(acc1) = c(colnames(acc),paste("tst",colnames(acc),sep="."))
  return(data.frame(id,acc1,stringsAsFactors = F)) 
}

start.time <- Sys.time()
res.var = lapply(sample,fcast.var)
end.time <- Sys.time()
time.taken <- end.time - start.time
res.var = res.var %>% rbind_all()
```

```{r}

library(parallel)
start.time <- Sys.time()
#http://gforge.se/2015/02/how-to-go-parallel-in-r-basics-tips/
no_cores <- detectCores() - 1
cl <- makeCluster(no_cores)
#required under windows
clusterExport(cl, "data.sample")
clusterExport(cl, "sample")
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(forecast))
clusterEvalQ(cl, library(zoo))
res = parLapply(cl,sample,fcast)
stopCluster(cl)
end.time <- Sys.time()
time.taken <- end.time - start.time
print(time.taken)
res = res %>% rbind_all()



par(mfrow=c(1,2))
boxplot(res$MAE)
boxplot(res.var$MAE)

```

